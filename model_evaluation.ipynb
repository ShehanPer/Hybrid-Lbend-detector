{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32777620",
   "metadata": {},
   "source": [
    "# Pattern-Based Object Detection for Bin Picking Robot Systems\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook presents a comprehensive evaluation and comparison of different **pattern-based object detection** methods specifically designed for **L-bend shaped objects** in **bin picking robot applications**. The project explores multiple template matching approaches to identify the most effective method for a Region Proposal Network (RPN) in automated industrial bin picking scenarios, ultimately combining the best-performing classical computer vision technique with modern deep learning validation.\n",
    "\n",
    "## Project Evolution and Methodology for Robotic Bin Picking\n",
    "\n",
    "### Experimental Phase: FFT Investigation for Industrial Applications\n",
    "\n",
    "Initially, **FFT-based template matching** was explored as a potential approach for bin picking robot vision systems:\n",
    "- High-pass filtering in the frequency domain to enhance edge features in cluttered bin environments\n",
    "- Normalized dot product (cosine similarity) for pattern matching in industrial settings\n",
    "- **Result**: While theoretically sound, FFT filtering did not provide reliable performance for bin picking robot RPN requirements, particularly in cluttered environments with partial occlusions\n",
    "\n",
    "### Production Methods: Classical + Modern Hybrid for Robotic Vision\n",
    "\n",
    "After experimentation, the final pipeline uses proven, robust approaches suitable for real-time robotic manipulation:\n",
    "\n",
    "1. **OpenCV Template Matching** (Primary RPN Method for Bin Picking)\n",
    "   - Standard `cv2.matchTemplate` with `TM_CCOEFF_NORMED`\n",
    "   - Multiple rotation angles for orientation independence in randomly placed objects\n",
    "   - Reliable, well-tested computer vision approach suitable for industrial robotics\n",
    "\n",
    "2. **PyTorch Batched Convolution** (High-Performance Alternative for GPU-enabled Robots)\n",
    "   - GPU-accelerated template matching using batched convolution\n",
    "   - Single-line convolution for all rotated templates simultaneously\n",
    "   - Optimized for real-time performance when GPU acceleration is available on robotic platforms\n",
    "\n",
    "3. **CNN Validation & Localization** (Precision Enhancement for Robot Grasping)\n",
    "   - Custom CNN architecture for L-bend object validation in bin picking scenarios\n",
    "   - Dual-head network: classification confidence + precise centroid regression for robot manipulation\n",
    "   - Filters false positives from template matching stage to prevent failed grasp attempts\n",
    "\n",
    "## Final Architecture for Bin Picking Systems\n",
    "\n",
    "```\n",
    "Robot Camera → Template Matching → Candidate Regions → CNN Validation → Robot Coordinates\n",
    "      ↓              ↓                    ↓                ↓                   ↓\n",
    "  Live Feed      OpenCV/PyTorch       Rotation-Invariant   Confidence +     Grasp Point\n",
    "  from Bin       Template Match       Bounding Boxes        Centroid        Coordinates\n",
    "```\n",
    "\n",
    "## Key Design Decisions for Robotic Applications\n",
    "\n",
    "- **FFT Approach Rejected**: Despite theoretical advantages, FFT-based filtering proved unreliable for practical bin picking RPN applications with cluttered environments\n",
    "- **OpenCV as Primary**: Chose battle-tested `cv2.matchTemplate` for consistent, predictable results in industrial robot deployments\n",
    "- **PyTorch for Speed**: Batched convolution approach when GPU acceleration is needed for high-throughput bin picking\n",
    "- **CNN Validation**: Added deep learning stage to reduce false positives and provide precise grasp point localization for robotic manipulation\n",
    "\n",
    "## Performance Characteristics for Industrial Deployment\n",
    "\n",
    "- **Reliability**: OpenCV template matching provides consistent, repeatable results for autonomous robot operation\n",
    "- **Speed**: PyTorch batched approach achieves >20 FPS on GPU-enabled robotic systems\n",
    "- **Accuracy**: CNN validation stage significantly reduces false positive rates, preventing failed grasp attempts\n",
    "- **Robustness**: Multi-angle template matching handles arbitrary object orientations in randomly filled bins\n",
    "\n",
    "## Applications in Robotic Automation\n",
    "\n",
    "This hybrid approach is particularly suitable for:\n",
    "- **Automated Bin Picking**: Primary application for industrial robot cells\n",
    "- **Quality Control Robotics**: Automated inspection and sorting of L-bend components\n",
    "- **Assembly Line Integration**: Real-time part recognition for robotic assembly systems\n",
    "- **Warehouse Automation**: Automated picking and sorting in fulfillment centers\n",
    "\n",
    "## Implementation Notes for Robotic Integration\n",
    "\n",
    "The notebook demonstrates:\n",
    "1. **FFT exploration** (initial research phase for robotic vision)\n",
    "2. **OpenCV implementation** (production-ready RPN for robot deployment)\n",
    "3. **PyTorch acceleration** (high-performance alternative for advanced robotic platforms)\n",
    "4. **CNN training pipeline** (validation and precise localization for robot grasping)\n",
    "5. **Real-time inference** (complete solution ready for robotic integration)\n",
    "6. **Model export** (TorchScript and ONNX for deployment on robotic controllers)\n",
    "7. **Coordinate transformation** (pixel-to-robot coordinate mapping for manipulation tasks)\n",
    "\n",
    "This comprehensive approach ensures both research completeness and practical deployment readiness for industrial bin picking robot systems, with clear documentation of why certain methods were chosen over others for robotic applications.\n",
    "\n",
    "## Real-World Deployment Considerations\n",
    "\n",
    "- **Hardware Compatibility**: Tested and optimized for NVIDIA Jetson Nano and similar embedded platforms commonly used in robotics\n",
    "- **Real-time Performance**: Achieves 15-20 FPS for responsive robot control\n",
    "- **Industrial Environment**: Robust to varying lighting conditions and cluttered bin scenarios\n",
    "- **Integration Ready**: Modular design allows easy integration with existing robot control systems and manipulation pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841c9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# This is a simple NMS implementation for averaging bounding boxes based on Intersection over Union (IoU).\n",
    "def average_boxes(boxes, iou_thresh=0.5):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    keep = []\n",
    "\n",
    "    while len(boxes) > 0:\n",
    "        ref_box = boxes[0]\n",
    "        rest = boxes[1:]\n",
    "\n",
    "        x1, y1, x2, y2, score = ref_box\n",
    "        ious = []\n",
    "\n",
    "        for box in rest:\n",
    "            xx1 = max(x1, box[0])\n",
    "            yy1 = max(y1, box[1])\n",
    "            xx2 = min(x2, box[2])\n",
    "            yy2 = min(y2, box[3])\n",
    "            inter = max(0, xx2 - xx1) * max(0, yy2 - yy1)\n",
    "            union = (x2 - x1) * (y2 - y1) + (box[2] - box[0]) * (box[3] - box[1]) - inter\n",
    "            iou = inter / union if union > 0 else 0\n",
    "            ious.append(iou)\n",
    "\n",
    "        ious = np.array([1.0] + ious)\n",
    "        cluster = boxes[np.where(ious >= iou_thresh)[0]]\n",
    "\n",
    "        avg_box = np.average(cluster, axis=0, weights=cluster[:, 4])  # Weighted by score\n",
    "        keep.append(avg_box)\n",
    "\n",
    "        boxes = np.delete(boxes, np.where(ious >= iou_thresh)[0], axis=0)\n",
    "\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17411727",
   "metadata": {},
   "source": [
    "## Using **FFT Filters** for patern matching\n",
    "\n",
    "\n",
    "\n",
    "This section demonstrates how to use FFT-based high-pass filtering and template matching for robust pattern detection, including rotation invariance.\n",
    "\n",
    "### **Workflow Overview**\n",
    "\n",
    "1. **High-Pass Filtering with FFT**\n",
    "    - The `high_pass_filter_fft` function applies a high-pass filter in the frequency domain to both the input image and the template. This enhances edges and suppresses low-frequency background, making the matching more robust to illumination changes.\n",
    "\n",
    "2. **Template Rotation**\n",
    "    - The `rotate_image_with_padding` function rotates the template to multiple angles, ensuring the rotated template fits within the new image bounds without cropping. This enables rotation-invariant detection.\n",
    "\n",
    "3. **Template Matching via Dot Product**\n",
    "    - The `convolve_and_get_bboxes` function slides each rotated template over the filtered image, computing the normalized dot product (cosine similarity) at each location. Locations with scores above a threshold are considered detections.\n",
    "\n",
    "4. **Non-Maximum Suppression (NMS)**\n",
    "    - The `non_max_suppression_fast` function removes overlapping detections, keeping only the highest-scoring bounding boxes.\n",
    "    - The `average_boxes` function (defined elsewhere) further merges boxes with high overlap to produce robust final detections.\n",
    "\n",
    "5. **Visualization**\n",
    "    - Detected bounding boxes and their scores are drawn on the image.\n",
    "    - The result is displayed and saved as `output.png`.\n",
    "\n",
    "### **Advantages**\n",
    "- **Edge/Pattern Focus:** High-pass filtering makes the method robust to lighting and background variations.\n",
    "- **Rotation Invariance:** By matching multiple rotated templates, the method detects objects regardless of their orientation.\n",
    "- **Customizable:** Thresholds and rotation angles can be tuned for different applications.\n",
    "\n",
    "### **Usage Notes**\n",
    "- Adjust the `threshold` in `convolve_and_get_bboxes` and the `iou_thresh` in NMS for your dataset.\n",
    "- Ensure the template and image are preprocessed (resized, filtered) consistently.\n",
    "- This method is suitable for detecting objects with strong edge or pattern features, especially when orientation varies.\n",
    "\n",
    "### **Example Results**\n",
    "\n",
    "<img src=\"Results/fft analysis.png\" alt=\"FFT Analysis Visualization\" width=\"350\"/>\n",
    "<img src=\"Results/fft_filtering_rpn.png\" alt=\"FFT Filtering RPN Visualization\" width=\"350\"/>\n",
    "<img src=\"Results/output.png\" alt=\"FFT Filtered Detection Example\" width=\"350\"/>\n",
    "\n",
    "*Detected bounding boxes are shown in green. The images demonstrate the effect of FFT-based filtering and successful detection of rotated templates using FFT-based high-pass filtering and template matching.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a69f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "# --- Step 1: High-pass filter and FFT helpers ---\n",
    "\n",
    "def high_pass_filter_fft(image):\n",
    "   \n",
    "    # Step 1: Forward FFT\n",
    "    f = np.fft.fft2(image)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "\n",
    "    # Step 2: Create High-Pass Mask\n",
    "    rows, cols = image.shape\n",
    "    crow, ccol = rows // 2, cols // 2\n",
    "\n",
    "    radius = min(rows, cols) // 10  # Adjust radius based on image size\n",
    "    radius = 8\n",
    "    mask = np.ones((rows, cols), np.uint8)\n",
    "    mask[crow - radius:crow + radius, ccol - radius:ccol + radius] = 0  # Suppress low frequencies\n",
    "\n",
    "    # Step 3: Apply mask and inverse FFT\n",
    "    fshift_filtered = fshift * mask\n",
    "    f_ishift = np.fft.ifftshift(fshift_filtered)\n",
    "    img_back = np.fft.ifft2(f_ishift)\n",
    "    img_back = np.abs(img_back)\n",
    "\n",
    "    # Step 4: Normalize for consistent scaling (0 to 1)\n",
    "    img_back = (img_back - np.min(img_back)) / (np.max(img_back) - np.min(img_back))\n",
    "\n",
    "    return img_back\n",
    "\n",
    "# --- Step 2: Rotate template with padding ---\n",
    "def rotate_image_with_padding(image, angle):\n",
    "    h, w = image.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    cos = np.abs(rot_mat[0, 0])\n",
    "    sin = np.abs(rot_mat[0, 1])\n",
    "    nW = int((h * sin) + (w * cos))\n",
    "    nH = int((h * cos) + (w * sin))\n",
    "\n",
    "    rot_mat[0, 2] += (nW / 2) - center[0]\n",
    "    rot_mat[1, 2] += (nH / 2) - center[1]\n",
    "\n",
    "    return cv2.warpAffine(image, rot_mat, (nW, nH), borderValue=255)\n",
    "\n",
    "# --- Step 3: Convolution with dot product ---\n",
    "def convolve_and_get_bboxes(image, template, threshold):\n",
    "    h, w = template.shape\n",
    "    ih, iw = image.shape\n",
    "    bboxes = []\n",
    "\n",
    "    stride_y = max(1, h // 20)\n",
    "    stride_x = max(1, w // 20)\n",
    "\n",
    "    for y in range(0, ih - h + 1, stride_y):\n",
    "        for x in range(0, iw - w + 1, stride_x):\n",
    "            patch = image[y:y+h, x:x+w]\n",
    "            dot = np.dot(patch.flatten(), template.flatten())\n",
    "            norm = np.linalg.norm(patch) * np.linalg.norm(template)\n",
    "            score = dot / (norm + 1e-6)\n",
    "            print(f\"Score at ({x}, {y}): {score:.4f} dot {dot} and norm {norm}\")  # Debugging output\n",
    "            if score >= threshold:\n",
    "                bboxes.append((x, y, x + w, y + h, score))\n",
    "    return bboxes\n",
    "\n",
    "# --- Step 4: Non-Maximum Suppression ---\n",
    "def non_max_suppression_fast(boxes, iou_thresh=0.3):\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = np.array(boxes)\n",
    "    x1 = boxes[:,0]; y1 = boxes[:,1]; x2 = boxes[:,2]; y2 = boxes[:,3]; scores = boxes[:,4]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "\n",
    "    keep = []\n",
    "    while order.size > 0:\n",
    "        i = order[0]\n",
    "        keep.append(tuple(boxes[i]))\n",
    "\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        inter = w * h\n",
    "        iou = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "\n",
    "        order = order[1:][iou <= iou_thresh]\n",
    "\n",
    "    return keep\n",
    "\n",
    "# --- Step 5: Main Pipeline ---\n",
    "def main():\n",
    "    \n",
    "    # Load and resize full image\n",
    "    img = cv2.imread('Dataset\\\\test\\\\images\\\\image_20250321_200236_lbent_3.png', cv2.IMREAD_GRAYSCALE)\n",
    "    img_resized = cv2.resize(img, (240, 240))\n",
    "    img_filtered = high_pass_filter_fft(img_resized)\n",
    "    #cv2.imshow(\"img\",img_filtered)\n",
    "\n",
    "    # Load and resize template\n",
    "    template = cv2.imread(\"image_20250321_201450_lbent_1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "    #template_filteredr= high_pass_filter_fft(template)\n",
    "    template_resized = cv2.resize(template, (64, 64))\n",
    "    template_filtered = high_pass_filter_fft(template_resized)\n",
    "    #cv2.imshow(\"temp\",template_filtered)\n",
    "    # Generate rotated templates\n",
    "    angles = [0,45,90,135, 180,225, 270,315]\n",
    "    rotated_templates = [rotate_image_with_padding(template_filtered, a) for a in angles]\n",
    "    start = time.time()\n",
    "    # Collect bboxes from all rotations\n",
    "    all_bboxes = []\n",
    "    for temp in rotated_templates:\n",
    "        bboxes = convolve_and_get_bboxes(img_filtered, temp, threshold=0.67)\n",
    "        all_bboxes.extend(bboxes)\n",
    "\n",
    "    # Apply NMS\n",
    "    final_bboxes = non_max_suppression_fast(all_bboxes, iou_thresh=0.3)\n",
    "    final_bboxes = average_boxes(final_bboxes, iou_thresh=0.1)\n",
    "    \n",
    "    end = time.time()\n",
    "    # Draw and show\n",
    "    out = cv2.cvtColor(img_resized, cv2.COLOR_GRAY2BGR)\n",
    "    for (x1, y1, x2, y2, score) in final_bboxes:\n",
    "        cv2.rectangle(out, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(out, f\"Score: {score: .2f}\", (int(x1), int(y1) - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (122, 0, 0), 2)\n",
    "\n",
    "    \n",
    "\n",
    "    print(f\"time = {end - start:.2f} seconds\")\n",
    "    print(f\"Number of detections: {len(final_bboxes)}\")  \n",
    "\n",
    "    cv2.imshow(\"Detections\", out)\n",
    "    cv2.imwrite(\"output.png\", out)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c80cf5",
   "metadata": {},
   "source": [
    "## Using **Opencv TemplateMatching** method\n",
    "\n",
    "This section demonstrates how to perform rotation-invariant template matching using OpenCV's `matchTemplate` method. The workflow includes rotating the template to multiple angles, matching each rotated template to the input image, and applying non-maximum suppression (NMS) to filter overlapping detections. The final bounding boxes are averaged for robustness.\n",
    "\n",
    "### **Key Steps**\n",
    "\n",
    "1. **Image and Template Preparation**\n",
    "    - Load the grayscale input image and template.\n",
    "    - Resize both for consistent processing.\n",
    "\n",
    "2. **Template Rotation**\n",
    "    - The `rotate_image_with_padding` function rotates the template by a given angle, ensuring the rotated template fits within the new image bounds without cropping.\n",
    "\n",
    "3. **Template Matching**\n",
    "    - For each rotation angle, the template is matched to the image using `cv2.matchTemplate` with the `TM_CCOEFF_NORMED` method.\n",
    "    - Detections above a threshold are collected as bounding boxes with scores.\n",
    "\n",
    "4. **Non-Maximum Suppression (NMS)**\n",
    "    - The `nms` function removes overlapping detections, keeping only the highest-scoring boxes.\n",
    "    - The `average_boxes` function further merges boxes with high overlap to produce robust final detections.\n",
    "\n",
    "5. **Visualization**\n",
    "    - Detected bounding boxes and their scores are drawn on the image.\n",
    "    - The result is displayed and saved.\n",
    "\n",
    "### **Advantages**\n",
    "- **Rotation Invariance:** By matching multiple rotated templates, the method detects objects regardless of their orientation.\n",
    "- **Simplicity:** Uses standard OpenCV functions, making it easy to adapt and extend.\n",
    "- **Post-processing:** NMS and box averaging improve detection quality by reducing duplicates.\n",
    "\n",
    "### **Usage Notes**\n",
    "- Adjust the `threshold` and `iou_thresh` parameters for your specific application and dataset.\n",
    "- Ensure the template and image are preprocessed (e.g., resized, normalized) consistently for best results.\n",
    "- This method is suitable for scenarios where the object of interest may appear at arbitrary rotations in the image.\n",
    "\n",
    "### **Example Results**\n",
    "\n",
    "<img src=\"Results/detections_output_batched_angle.png\" alt=\"Detection Example\" width=\"350\"/>\n",
    "\n",
    "*Detected bounding boxes are shown in green. The image demonstrates successful detection of rotated templates using OpenCV's template matching with rotation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfe162df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 0.13 sec\n",
      "Detections: 5\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "# Rotate image with padding\n",
    "def rotate_image_with_padding(image, angle):\n",
    "    h, w = image.shape\n",
    "    center = (w // 2, h // 2)\n",
    "    rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    cos, sin = np.abs(rot_mat[0, 0]), np.abs(rot_mat[0, 1])\n",
    "    nW, nH = int(h * sin + w * cos), int(h * cos + w * sin)\n",
    "    rot_mat[0, 2] += (nW / 2) - center[0]\n",
    "    rot_mat[1, 2] += (nH / 2) - center[1]\n",
    "    return cv2.warpAffine(image, rot_mat, (nW, nH), borderValue=255)\n",
    "\n",
    "# Template matching with score threshold\n",
    "def match_template(image, template, threshold=0.5):\n",
    "    result = cv2.matchTemplate(image, template, cv2.TM_CCOEFF_NORMED)\n",
    "    yx = np.where(result >= threshold)\n",
    "    h, w = template.shape\n",
    "    return [(x, y, x + w, y + h, result[y, x]) for y, x in zip(*yx)]\n",
    "\n",
    "# Non-Maximum Suppression\n",
    "def nms(boxes, iou_thresh=0.3):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    boxes = np.array(boxes)\n",
    "    x1, y1, x2, y2, scores = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]\n",
    "    areas = (x2 - x1) * (y2 - y1)\n",
    "    order = scores.argsort()[::-1]\n",
    "    keep = []\n",
    "\n",
    "    while order.size:\n",
    "        i = order[0]\n",
    "        keep.append(tuple(boxes[i]))\n",
    "        xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "        yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "        xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "        yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "        w = np.maximum(0.0, xx2 - xx1)\n",
    "        h = np.maximum(0.0, yy2 - yy1)\n",
    "        iou = (w * h) / (areas[i] + areas[order[1:]] - (w * h) + 1e-6)\n",
    "        order = order[1:][iou <= iou_thresh]\n",
    "\n",
    "    return keep\n",
    "\n",
    "# Main function\n",
    "def main():\n",
    "    start = time.time()\n",
    "\n",
    "    img = cv2.imread('Dataset/test/images/image_20250321_200736_lbent_3.png', cv2.IMREAD_GRAYSCALE)\n",
    "    template = cv2.imread(\"image_20250321_201450_lbent_1.png\", cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    img = cv2.resize(img, (260, 260))\n",
    "    template = cv2.resize(template, (64,64))\n",
    "\n",
    "    angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "    all_boxes = []\n",
    "\n",
    "    for angle in angles:\n",
    "        rotated = rotate_image_with_padding(template, angle)\n",
    "        bboxes = match_template(img, rotated, threshold=0.48)\n",
    "        all_boxes.extend(bboxes)\n",
    "\n",
    "    final_boxes = nms(all_boxes, iou_thresh=0.6)\n",
    "    final_boxes = average_boxes(all_boxes,iou_thresh=0.22)\n",
    "\n",
    "    output = cv2.cvtColor(img, cv2.COLOR_GRAY2BGR)\n",
    "    for x1, y1, x2, y2, score in final_boxes:\n",
    "        cv2.rectangle(output, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(output, f\"{score:.2f}\", (int(x1), int(y1) - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (222, 0, 0), 1)\n",
    "\n",
    "    print(f\"Time taken: {time.time() - start:.2f} sec\")\n",
    "    print(f\"Detections: {len(final_boxes)}\")\n",
    "\n",
    "    cv2.imshow(\"Detections\", output)\n",
    "    cv2.imwrite(\"detections_output.png\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04a6d33",
   "metadata": {},
   "source": [
    "This section demonstrates a fast, rotation-invariant template matching pipeline using PyTorch and a single batched convolution line. The approach leverages GPU acceleration for efficient detection of rotated templates in an image.\n",
    "\n",
    "### **Key Steps and Structure**\n",
    "\n",
    "1. **Imports and Setup**\n",
    "    - Uses `cv2` for image I/O and resizing, `numpy` for array operations, and `torch` for tensor computations.\n",
    "    - `torch.nn.functional` and `torchvision.ops.nms` are used for convolution and non-maximum suppression (NMS).\n",
    "\n",
    "2. **Image Preprocessing**\n",
    "    - `to_tensor_from_red_channel(img_bgr)`: Converts the red channel of a BGR image to a normalized PyTorch tensor of shape `[1, 1, H, W]`.\n",
    "\n",
    "3. **Template Rotation**\n",
    "    - `rotate_tensor_image(tensor_img, angle_deg)`: Rotates a tensor image by a specified angle using affine transformations.\n",
    "    - `get_rotated_templates(template_tensor, angles)`: Generates a batch of rotated templates for all specified angles, normalizing each.\n",
    "\n",
    "4. **Batched Template Matching**\n",
    "    - `match_template_batched(image_tensor, templates_batch, threshold)`: Performs a single batched `conv2d` operation to match all rotated templates at once. Detections above the threshold are collected as bounding boxes with scores.\n",
    "\n",
    "5. **Non-Maximum Suppression**\n",
    "    - `nms_torch(boxes, iou_threshold)`: Applies NMS to remove overlapping detections, keeping only the best matches.\n",
    "\n",
    "6. **Main Pipeline**\n",
    "    - Loads and resizes the input image and template.\n",
    "    - Converts both to tensors using only the red channel.\n",
    "    - Generates rotated templates for multiple angles.\n",
    "    - Runs batched template matching and NMS to get final detections.\n",
    "    - Draws bounding boxes and scores on the output image and displays/saves the result.\n",
    "\n",
    "### **Advantages**\n",
    "- **Speed:** All template rotations are matched in a single convolution line, fully utilizing GPU parallelism.\n",
    "- **Rotation Invariance:** By matching multiple rotated templates, the method detects objects regardless of their orientation.\n",
    "- **Simplicity:** The code is concise and easy to adapt for other templates or image sizes.\n",
    "\n",
    "### **Usage**\n",
    "- Replace image paths as needed.\n",
    "- Ensure a CUDA-capable GPU is available for best performance.\n",
    "- Adjust the detection threshold and angles for your specific application.\n",
    "\n",
    "This approach is ideal for real-time or batch detection tasks where both speed and rotation invariance are required.\n",
    "\n",
    "### **Example Results**\n",
    "\n",
    "Below are some example detections using the PyTorch batched convolution approach:\n",
    "\n",
    "<img src=\"Results/detections_output_batched.png\" alt=\"Detection Example 1\" width=\"320\"/>\n",
    "<img src=\"Results/Screenshot 2025-05-25 154030.png\" alt=\"Detection Example 2\" width=\"320\"/>\n",
    "\n",
    "*Detected bounding boxes are shown in green. Each image demonstrates successful detection of rotated templates using a single batched convolution.*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aedcc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "import time\n",
    "import math\n",
    "\n",
    "# Convert red-channel grayscale image to normalized PyTorch tensor\n",
    "def to_tensor_from_red_channel(img_bgr):\n",
    "    red_channel = img_bgr[:, :, 2].astype(np.float32) / 255.0  # Normalize to [0, 1]\n",
    "    return torch.from_numpy(red_channel).unsqueeze(0).unsqueeze(0).contiguous()  # [1, 1, H, W]\n",
    "\n",
    "# Rotate a single image tensor\n",
    "def rotate_tensor_image(tensor_img, angle_deg):\n",
    "    angle_rad = math.radians(angle_deg)\n",
    "    theta = torch.tensor([\n",
    "        [math.cos(angle_rad), -math.sin(angle_rad), 0],\n",
    "        [math.sin(angle_rad),  math.cos(angle_rad), 0]\n",
    "    ], dtype=torch.float32, device=tensor_img.device)\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), tensor_img.size(), align_corners=False)\n",
    "    rotated = F.grid_sample(tensor_img, grid, padding_mode='zeros', align_corners=False)\n",
    "    return rotated\n",
    "\n",
    "# Rotate template batch\n",
    "def get_rotated_templates(template_tensor, angles):\n",
    "    rotated_templates = []\n",
    "    for angle in angles:\n",
    "        rotated = rotate_tensor_image(template_tensor, angle)\n",
    "        rotated -= rotated.mean()  # Normalize each rotated template\n",
    "        rotated_templates.append(rotated)\n",
    "    return torch.cat(rotated_templates, dim=0)  # [N, 1, h, w]\n",
    "\n",
    "# Batched template matching using conv2d\n",
    "def match_template_batched(image_tensor, templates_batch, threshold=0.48):\n",
    "    N, _, h, w = templates_batch.shape\n",
    "    response = F.conv2d(image_tensor, templates_batch,stride=1)  # [1, N, H−h+1, W−w+1]\n",
    "    response_np = response.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    all_boxes = []\n",
    "    for i in range(N):\n",
    "        r = response_np[i]\n",
    "        yx = np.where(r >= threshold)\n",
    "        for y, x in zip(*yx):\n",
    "            score = r[y, x]\n",
    "            all_boxes.append((x, y, x + w, y + h, score))\n",
    "            #print(f\"Template {i}, Score at ({x}, {y}): {score:.4f}\")  # Debug output\n",
    "    return all_boxes\n",
    "\n",
    "# Torchvision NMS\n",
    "def nms_torch(boxes, iou_threshold=0.6):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    boxes_np = np.array(boxes)\n",
    "    boxes_tensor = torch.tensor(boxes_np[:, :4], dtype=torch.float32)\n",
    "    scores_tensor = torch.tensor(boxes_np[:, 4], dtype=torch.float32)\n",
    "    keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold)\n",
    "    return [(*boxes_np[i][:4], boxes_np[i][4]) for i in keep_indices]\n",
    "\n",
    "# MAIN\n",
    "def main():\n",
    "    start = time.time()\n",
    "\n",
    "    # Load color images and convert using red channel only\n",
    "    img_bgr = cv2.imread('Dataset/test/images/image_20250321_200736_lbent_3.png')\n",
    "    template_bgr = cv2.imread('image_20250321_201450_lbent_1.png')\n",
    "    img_bgr = cv2.resize(img_bgr, (228, 228))\n",
    "    template_bgr = cv2.resize(template_bgr, (64, 64))\n",
    "\n",
    "    image_tensor = to_tensor_from_red_channel(img_bgr).cuda()\n",
    "    template_tensor = to_tensor_from_red_channel(template_bgr).cuda()\n",
    "\n",
    "    angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "    rotated_templates = get_rotated_templates(template_tensor, angles)  # [N, 1, h, w]\n",
    "\n",
    "    all_boxes = match_template_batched(image_tensor, rotated_templates, threshold=80)\n",
    "    final_boxes = nms_torch(all_boxes, iou_threshold=0.0)\n",
    "\n",
    "    print(f\"Time taken: {time.time() - start:.2f} sec\")\n",
    "\n",
    "    output = cv2.cvtColor((image_tensor.squeeze().cpu().numpy() * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "    for x1, y1, x2, y2, score in final_boxes:\n",
    "        cv2.rectangle(output, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "        cv2.putText(output, f\"{score:.2f}\", (int(x1), int(y1) - 5),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 100), 1)\n",
    "\n",
    "    \n",
    "    print(f\"Detections: {len(final_boxes)}\")\n",
    "\n",
    "    cv2.imshow(\"Detections\", output)\n",
    "    cv2.imwrite(\"detections_output_batched.png\", output)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59530672",
   "metadata": {},
   "source": [
    "## Real time inference\n",
    "This section demonstrates a real-time template matching pipeline using PyTorch and OpenCV. The approach leverages fast batched convolution on the GPU to detect rotated instances of a template in live video frames.\n",
    "\n",
    "**Key steps:**\n",
    "- The template image is loaded, resized, and converted to a PyTorch tensor using only the red channel for simplicity.\n",
    "- Multiple rotated versions of the template are generated to achieve rotation-invariant detection.\n",
    "- For each video frame, the frame is resized and converted to a tensor, then matched against all rotated templates using a single batched `conv2d` operation.\n",
    "- Detections above a threshold are filtered using Non-Maximum Suppression (NMS) to remove duplicates.\n",
    "- Detected bounding boxes are drawn on the frame, and the result is displayed in real time with FPS information.\n",
    "\n",
    "This method is efficient because it uses GPU-accelerated convolution for template matching and supports real-time inference on live video streams. It is suitable for applications where you need to detect specific patterns or objects (with rotation invariance) in a video feed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0117535c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Press 'q' to quit.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torchvision.ops import nms\n",
    "import time\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")  # Force CPU for compatibility\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "def to_tensor_from_red_channel(img_bgr):\n",
    "    red_channel = img_bgr[:, :, 2].astype(np.float32) / 255.0\n",
    "    return torch.from_numpy(red_channel).unsqueeze(0).unsqueeze(0).contiguous()\n",
    "\n",
    "def rotate_tensor_image(tensor_img, angle_deg):\n",
    "    angle_rad = math.radians(angle_deg)\n",
    "    theta = torch.tensor([\n",
    "        [math.cos(angle_rad), -math.sin(angle_rad), 0],\n",
    "        [math.sin(angle_rad),  math.cos(angle_rad), 0]\n",
    "    ], dtype=torch.float32, device=tensor_img.device)\n",
    "    grid = F.affine_grid(theta.unsqueeze(0), tensor_img.size(), align_corners=False)\n",
    "    rotated = F.grid_sample(tensor_img, grid, padding_mode='zeros', align_corners=False)\n",
    "    return rotated\n",
    "\n",
    "def get_rotated_templates(template_tensor, angles):\n",
    "    rotated_templates = []\n",
    "    for angle in angles:\n",
    "        rotated = rotate_tensor_image(template_tensor, angle)\n",
    "        rotated -= rotated.mean()\n",
    "        rotated_templates.append(rotated)\n",
    "    return torch.cat(rotated_templates, dim=0)\n",
    "\n",
    "def match_template_batched(image_tensor, templates_batch, threshold=0.5):\n",
    "    N, _, h, w = templates_batch.shape\n",
    "    response = F.conv2d(image_tensor, templates_batch, stride=1)\n",
    "    response_np = response.squeeze(0).detach().cpu().numpy()\n",
    "\n",
    "    all_boxes = []\n",
    "    for i in range(N):\n",
    "        r = response_np[i]\n",
    "        yx = np.where(r >= threshold)\n",
    "        for y, x in zip(*yx):\n",
    "            score = r[y, x]\n",
    "            all_boxes.append((x, y, x + w, y + h, score))\n",
    "    return all_boxes\n",
    "\n",
    "\n",
    "\n",
    "def nms_torch(boxes, iou_threshold=0.6):\n",
    "    if not boxes:\n",
    "        return []\n",
    "    boxes_np = np.array(boxes)\n",
    "    boxes_tensor = torch.tensor(boxes_np[:, :4], dtype=torch.float32)\n",
    "    scores_tensor = torch.tensor(boxes_np[:, 4], dtype=torch.float32)\n",
    "    keep_indices = nms(boxes_tensor, scores_tensor, iou_threshold)\n",
    "    return [(*boxes_np[i][:4], boxes_np[i][4]) for i in keep_indices]\n",
    "\n",
    "def main():\n",
    "    template_bgr = cv2.imread('image_20250321_201450_lbent_1.png')\n",
    "    template_bgr = cv2.resize(template_bgr, (84, 84))\n",
    "    template_tensor = to_tensor_from_red_channel(template_bgr).to(device)\n",
    "    angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "    rotated_templates = get_rotated_templates(template_tensor, angles).to(device)\n",
    "\n",
    "    url = 'http://192.168.8.104:8080/video'  # Replace with your IP camera stream\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Unable to open video stream at {url}\")\n",
    "        return\n",
    "\n",
    "    print(\"Press 'q' to quit.\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame.\")\n",
    "            break\n",
    "\n",
    "        frame = cv2.resize(frame, (170, 170))\n",
    "        image_tensor = to_tensor_from_red_channel(frame).to(device)\n",
    "\n",
    "        start = time.time()\n",
    "        all_boxes = match_template_batched(image_tensor, rotated_templates, threshold=80)\n",
    "        final_boxes = nms_torch(all_boxes, iou_threshold=0.01)\n",
    "        \n",
    "\n",
    "        output = cv2.cvtColor((image_tensor.squeeze().cpu().numpy() * 255).astype(np.uint8), cv2.COLOR_GRAY2BGR)\n",
    "        for x1, y1, x2, y2, score in final_boxes:\n",
    "            cv2.rectangle(output, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
    "            cv2.putText(output, f\"{score:.2f}\", (int(x1), int(y1) - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 100), 1)\n",
    "        elapsed = time.time() - start\n",
    "        if elapsed > 0:\n",
    "            \n",
    "            cv2.putText(output, f\"FPS: {1/elapsed:.2f}\", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "            cv2.putText(output,f\"time: {elapsed:.2f} sec\", (5, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 1)\n",
    "        cv2.imshow(\"Real-Time Detection\", output)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with torch.no_grad():\n",
    "        main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9adebdd8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6892a33f",
   "metadata": {},
   "source": [
    "## Make new cropped data set\n",
    "\n",
    "This script creates a new dataset of cropped images based on COCO-style annotations. It reads bounding box annotations from a JSON file, loads each corresponding image, and crops a region around each annotated object (with optional padding). The cropped region is resized to a fixed size (96x96 pixels) and saved to an output directory. This process is useful for generating training data for object detection or classification tasks, focusing on the regions of interest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a592c6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "\n",
    "# CONFIG\n",
    "json_path = 'dataset01\\\\annotations\\\\train.json'\n",
    "images_dir = 'dataset01\\\\train'  # UPDATE THIS\n",
    "output_dir = 'cropped_l_bends'\n",
    "crop_size = 96\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "with open(json_path, 'r') as f:\n",
    "    coco = json.load(f)\n",
    "\n",
    "# Index images by id\n",
    "id_to_filename = {img['id']: img['file_name'] for img in coco['images']}\n",
    "\n",
    "for ann in coco['annotations']:\n",
    "    img_id = ann['image_id']\n",
    "    bbox = ann['bbox']  # [x, y, w, h]\n",
    "\n",
    "    x, y, w, h = bbox\n",
    "    cx, cy = x + w / 2, y + h / 2  # centroid in original image\n",
    "\n",
    "    image_path = os.path.join(images_dir, id_to_filename[img_id])\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    if image is None:\n",
    "        print(f\"Warning: Could not load {image_path}\")\n",
    "        continue\n",
    "\n",
    "    H, W, _ = image.shape\n",
    "\n",
    "    # Crop a square region centered at (cx, cy)\n",
    "    \n",
    "    cx, cy = int(cx), int(cy)\n",
    "    # Convert bbox coordinates to integers for slicing\n",
    "\n",
    "    x1, y1 = int(x), int(y)\n",
    "    x2, y2 = int(x + w), int(y + h)\n",
    "    #add padding befor cropping\n",
    "    pad = 5\n",
    "    x1 = max(0,x1 - pad)\n",
    "    y1 = max(0,y1 - pad)\n",
    "    x2 = min(W, x2 + pad)\n",
    "    y2 = min(H, y2 + pad)\n",
    "\n",
    "\n",
    "    # Handle border cases\n",
    "    if x1 < 0 or y1 < 0 or x2 > W or y2 > H:\n",
    "        continue  # skip if crop goes out of bounds\n",
    "\n",
    "    crop = image[y1:y2, x1:x2]\n",
    "    resized = cv2.resize(crop, (crop_size, crop_size))\n",
    "\n",
    "    out_name = f\"l_bend_{ann['id']:04d}.png\"\n",
    "    out_path = os.path.join(output_dir, out_name)\n",
    "    cv2.imwrite(out_path, resized)\n",
    "\n",
    "    print(f\"Saved: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8fc745",
   "metadata": {},
   "source": [
    "## CNN model for validate objects and cetroid predict\n",
    "\n",
    "This section defines a PyTorch-based pipeline for detecting and localizing \"L-bend\" objects in images using a convolutional neural network (CNN). The workflow includes:\n",
    "\n",
    "- **Custom Dataset (`LBendDataset`)**: Loads images and their annotations (labels and centroid coordinates) from a CSV file. Each sample returns the image tensor, a binary label (L-bend or not), and normalized centroid coordinates.\n",
    "\n",
    "- **CNN Model (`CNNDetector`)**: A simple CNN backbone extracts features from the input image. The network has two heads:\n",
    "    - A classification head (outputs probability of L-bend).\n",
    "    - A regression head (predicts centroid coordinates, normalized to [0,1]).\n",
    "\n",
    "- **Training Function (`train_model`)**: Handles data splitting, augmentation, model training, validation, and checkpointing. It optimizes both classification (binary cross-entropy) and centroid regression (MSE) losses. The best model is saved and exported to TorchScript for efficient inference.\n",
    "\n",
    "This approach enables both object validation (is it an L-bend?) and precise localization (where is the centroid?) in a single, end-to-end trainable model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28a209df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "\n",
    "# Make the dataset\n",
    "\n",
    "class LBendDataset(Dataset):\n",
    "    def __init__(self,csv_path,images_dir, transform=None):\n",
    "        self.annotations = pd.read_csv(csv_path, header=None)\n",
    "        self.annotations.columns = ['label_name', 'centroid_x', 'centroid_y', 'image_name', 'width', 'height']\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        #get image file name and path\n",
    "        img_name = self.annotations.iloc[idx]['image_name']\n",
    "        img_path = os.path.join(self.images_dir,img_name)\n",
    "\n",
    "        #load image\n",
    "        image = cv2.imread(img_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Could not load image at {img_path}\")\n",
    "        \n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        label = self.annotations.iloc[idx]['label_name']\n",
    "        if label == 'L_bent':\n",
    "            label = 1.0\n",
    "        else:\n",
    "            label = 0.0\n",
    "\n",
    "        h,w = image.shape[:2]\n",
    "        centroid_x = self.annotations.iloc[idx]['centroid_x']/w\n",
    "        centroid_y = self.annotations.iloc[idx]['centroid_y']/h\n",
    "        \n",
    "        #Resize if need a standard size\n",
    "        image = cv2.resize(image,(64,64))\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = image.astype(np.float32)/255.0 # change this isf need float16\n",
    "            image = torch.from_numpy(image).permute(2,0,1)\n",
    "\n",
    "        return(\n",
    "            image,\n",
    "            torch.tensor([float(label)],dtype=torch.float32),  # Assuming label is a float (e.g., 0.0 or 1.0 for binary classification)\n",
    "            torch.tensor([centroid_x, centroid_y], dtype=torch.float32),  # Centroid coordinates\n",
    "        )\n",
    "\n",
    "\n",
    "# Define the CNN model\n",
    "class CNNDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNDetector,self).__init__()\n",
    "\n",
    "        #Backbone \n",
    "        self.features = nn.Sequential(\n",
    "            #Layer 1\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(inplace=True),\n",
    "\n",
    "            #Layer 2\n",
    "            nn.Conv2d(16,32,kernel_size=3,stride = 1,padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            #Layer 3\n",
    "            nn.Conv2d(32,64,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(64), \n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            #Layer 4\n",
    "            nn.Conv2d(64,128,kernel_size=3,stride=1,padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),\n",
    "\n",
    "            nn.AdaptiveAvgPool2d(1)\n",
    "\n",
    "        )\n",
    "\n",
    "        #Classifier\n",
    "        self.classifier = nn.Linear(128,1)\n",
    "        #Regression head for centroid coordinates\n",
    "        self.regression = nn.Linear(128, 2)  # Output 2 values\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        #Extract features\n",
    "        features = self.features(x)\n",
    "        features = torch.flatten(features, 1)  # Flatten the features\n",
    "\n",
    "        class_output = torch.sigmoid(self.classifier(features))\n",
    "        centroid_output = torch.sigmoid(self.regression(features))\n",
    "\n",
    "        return class_output, centroid_output\n",
    "    \n",
    "def train_model(csv_path,images_dir,epochs = 30,batch_size = 32, learning_rate=0.001):\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Define transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    train_df,val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[0])\n",
    "    \n",
    "    train_csv = 'train_annotations.csv'\n",
    "    val_csv = 'val_annotations.csv'\n",
    "    train_df.to_csv(train_csv, index=False, header=False)\n",
    "    val_df.to_csv(val_csv, index=False, header=False)\n",
    "\n",
    "    train_dataset = LBendDataset(train_csv, images_dir, transform=None)\n",
    "    val_dataset = LBendDataset(val_csv, images_dir, transform=None)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset,batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "    model = CNNDetector().to(device)\n",
    "\n",
    "    #loss functions\n",
    "    classification_loss = nn.BCELoss()\n",
    "    regression_loss = nn.MSELoss()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_cls_loss = 0.0\n",
    "        train_reg_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for images,labels,centroids in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            centroids = centroids.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            class_output, centroid_output = model(images)\n",
    "\n",
    "            cls_loss = classification_loss(class_output.squeeze(), labels.squeeze())\n",
    "            reg_loss = regression_loss(centroid_output, centroids)\n",
    "\n",
    "            loss = cls_loss + 5.0*reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_cls_loss += cls_loss.item()\n",
    "            train_reg_loss += reg_loss.item()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        #Avg training loss\n",
    "        train_cls_loss /= len(train_loader)\n",
    "        train_reg_loss /= len(train_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        #validate model \n",
    "        model.eval()\n",
    "        val_cls_loss = 0.0\n",
    "        val_reg_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "             for images, labels, centroids in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                centroids = centroids.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                cls_output, centroid_output = model(images)\n",
    "                \n",
    "                # Calculate losses\n",
    "                cls_loss = classification_loss(cls_output, labels)\n",
    "                reg_loss = regression_loss(centroid_output, centroids)\n",
    "                loss = cls_loss + 5.0 * reg_loss\n",
    "                \n",
    "                # Accumulate validation loss\n",
    "                val_cls_loss += cls_loss.item()\n",
    "                val_reg_loss += reg_loss.item()\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predicted = (cls_output > 0.5).float()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        # Calculate average validation loss and accuracy\n",
    "        val_cls_loss /= len(val_loader)\n",
    "        val_reg_loss /= len(val_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} (Cls: {train_cls_loss:.4f}, Reg: {train_reg_loss:.4f})\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} (Cls: {val_cls_loss:.4f}, Reg: {val_reg_loss:.4f})\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(\"Saved best model!\")\n",
    "            \n",
    "            # Save the model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': accuracy,\n",
    "            }, 'best_lbend_model.pth')\n",
    "    \n",
    "    # Clean up temporary CSV files\n",
    "    os.remove(train_csv)\n",
    "    os.remove(val_csv)\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Export to TorchScript for faster inference\n",
    "    model.eval()\n",
    "    example = torch.rand(1, 3, 64, 64).to(device)\n",
    "    traced_model = torch.jit.trace(model, example)\n",
    "    traced_model.save('lbend_detector_scripted.pt')\n",
    "    print(\"Model exported to TorchScript format for fast inference\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb61ecc",
   "metadata": {},
   "source": [
    "### **Training process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71702461",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"labels_my-project-name_2025-07-02-05-15-40.csv\"\n",
    "images_dir = \"cropped_l_bends\"\n",
    "\n",
    "\n",
    "train_model(csv_path, images_dir, epochs=40, batch_size=16, learning_rate=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c715d10f",
   "metadata": {},
   "source": [
    "# Traaining and visualizing losses and accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7caae17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "def train_model(csv_path, images_dir, epochs=40, batch_size=16, learning_rate=0.001):\n",
    "    # Create lists to store metrics for plotting\n",
    "    train_losses = []\n",
    "    train_cls_losses = []\n",
    "    train_reg_losses = []\n",
    "    val_losses = []\n",
    "    val_cls_losses = []\n",
    "    val_reg_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define transformations\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split data and create datasets/loaders (your existing code)\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[0])\n",
    "    \n",
    "    train_csv = 'train_annotations.csv'\n",
    "    val_csv = 'val_annotations.csv'\n",
    "    train_df.to_csv(train_csv, index=False, header=False)\n",
    "    val_df.to_csv(val_csv, index=False, header=False)\n",
    "    \n",
    "    train_dataset = LBendDataset(train_csv, images_dir, transform=None)\n",
    "    val_dataset = LBendDataset(val_csv, images_dir, transform=None)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "    \n",
    "    # Initialize model, loss functions and optimizer\n",
    "    model = CNNDetector().to(device)\n",
    "    classification_loss = nn.BCELoss()\n",
    "    regression_loss = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
    "    \n",
    "    # Create a figure for real-time plotting\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    plt.ion()  # Turn on interactive mode\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_cls_loss = 0.0\n",
    "        train_reg_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        # Training phase\n",
    "        for images, labels, centroids in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            centroids = centroids.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            class_output, centroid_output = model(images)\n",
    "            \n",
    "            cls_loss = classification_loss(class_output.squeeze(), labels.squeeze())\n",
    "            reg_loss = regression_loss(centroid_output, centroids)\n",
    "            \n",
    "            loss = cls_loss + 5.0 * reg_loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_cls_loss += cls_loss.item()\n",
    "            train_reg_loss += reg_loss.item()\n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Calculate average training loss\n",
    "        train_cls_loss /= len(train_loader)\n",
    "        train_reg_loss /= len(train_loader)\n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Store training metrics\n",
    "        train_losses.append(train_loss)\n",
    "        train_cls_losses.append(train_cls_loss)\n",
    "        train_reg_losses.append(train_reg_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_cls_loss = 0.0\n",
    "        val_reg_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for images, labels, centroids in val_loader:\n",
    "                images = images.to(device)\n",
    "                labels = labels.to(device)\n",
    "                centroids = centroids.to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                cls_output, centroid_output = model(images)\n",
    "                \n",
    "                # Calculate losses\n",
    "                cls_loss = classification_loss(cls_output, labels)\n",
    "                reg_loss = regression_loss(centroid_output, centroids)\n",
    "                loss = cls_loss + 5.0 * reg_loss\n",
    "                \n",
    "                # Accumulate validation loss\n",
    "                val_cls_loss += cls_loss.item()\n",
    "                val_reg_loss += reg_loss.item()\n",
    "                val_loss += loss.item()\n",
    "                \n",
    "                # Calculate accuracy\n",
    "                predicted = (cls_output > 0.5).float()\n",
    "                correct += (predicted == labels).sum().item()\n",
    "                total += labels.size(0)\n",
    "        \n",
    "        # Calculate average validation metrics\n",
    "        val_cls_loss /= len(val_loader)\n",
    "        val_reg_loss /= len(val_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        accuracy = correct / total\n",
    "        \n",
    "        # Store validation metrics\n",
    "        val_losses.append(val_loss)\n",
    "        val_cls_losses.append(val_cls_loss)\n",
    "        val_reg_losses.append(val_reg_loss)\n",
    "        val_accuracies.append(accuracy)\n",
    "        \n",
    "        # Update learning rate based on validation loss\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f} (Cls: {train_cls_loss:.4f}, Reg: {train_reg_loss:.4f})\")\n",
    "        print(f\"Val Loss: {val_loss:.4f} (Cls: {val_cls_loss:.4f}, Reg: {val_reg_loss:.4f})\")\n",
    "        print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(\"Saved best model!\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'val_loss': val_loss,\n",
    "                'val_accuracy': accuracy,\n",
    "                'train_losses': train_losses,\n",
    "                'val_losses': val_losses,\n",
    "                'train_cls_losses': train_cls_losses,\n",
    "                'val_cls_losses': val_cls_losses,\n",
    "                'train_reg_losses': train_reg_losses,\n",
    "                'val_reg_losses': val_reg_losses,\n",
    "                'val_accuracies': val_accuracies\n",
    "            }, 'best_lbend_model.pth')\n",
    "    \n",
    "    # Final plot (non-interactive)\n",
    "    plt.ioff()\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Plot final losses\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.plot(range(1, epochs+1), train_losses, 'b-', label='Training')\n",
    "    plt.plot(range(1, epochs+1), val_losses, 'r-', label='Validation')\n",
    "    plt.title('Total Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    # Plot final classification losses\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.plot(range(1, epochs+1), train_cls_losses, 'b-', label='Training')\n",
    "    plt.plot(range(1, epochs+1), val_cls_losses, 'r-', label='Validation')\n",
    "    plt.title('Classification Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    # Plot final regression losses\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.plot(range(1, epochs+1), train_reg_losses, 'b-', label='Training')\n",
    "    plt.plot(range(1, epochs+1), val_reg_losses, 'r-', label='Validation')\n",
    "    plt.title('Regression Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    \n",
    "    # Plot final validation accuracy\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.plot(range(1, epochs+1), val_accuracies, 'g-')\n",
    "    plt.title('Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(True)\n",
    "    plt.gca().xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final_training_results.png', dpi=200)\n",
    "    plt.show()\n",
    "    \n",
    "    # Clean up temporary CSV files\n",
    "    os.remove(train_csv)\n",
    "    os.remove(val_csv)\n",
    "    \n",
    "    # Load the best model state\n",
    "    if best_model_state:\n",
    "        model.load_state_dict(best_model_state)\n",
    "    \n",
    "    # Export to TorchScript for faster inference\n",
    "    model.eval()\n",
    "    example = torch.rand(1, 3, 64, 64).to(device)\n",
    "    traced_model = torch.jit.trace(model, example)\n",
    "    traced_model.save('lbend_detector_scripted.pt')\n",
    "    print(\"Model exported to TorchScript format for fast inference\")\n",
    "    \n",
    "    # Return the trained model and metrics for further analysis\n",
    "    return model, {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_cls_losses': train_cls_losses,\n",
    "        'val_cls_losses': val_cls_losses,\n",
    "        'train_reg_losses': train_reg_losses,\n",
    "        'val_reg_losses': val_reg_losses,\n",
    "        'val_accuracies': val_accuracies\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    csv_path = \"labels_my-project-name_2025-07-02-05-15-40.csv\"\n",
    "    images_dir = \"cropped_l_bends\"\n",
    "    model = train_model(csv_path, images_dir)\n",
    "    \n",
    "    print(\"\\nTraining complete!\")\n",
    "    print(\"To view TensorBoard logs, run:\")\n",
    "    print(\"tensorboard --logdir=runs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1178aaf",
   "metadata": {},
   "source": [
    "## Model Evaluation and Visualization\n",
    "\n",
    "This code provides a comprehensive evaluation pipeline for the trained L-bend detector model. It includes:\n",
    "\n",
    "- **Model Loading**: Automatically loads either a TorchScript model (`.pt`) or a PyTorch checkpoint (`.pth`), supporting both fast inference and flexible retraining.\n",
    "\n",
    "- **Validation Data Preparation**: Splits the labeled dataset into training and validation sets, then creates a validation dataset and DataLoader for efficient batch processing.\n",
    "\n",
    "- **Evaluation Metrics**:\n",
    "    - **Classification Metrics**: Computes accuracy, precision, recall, F1-score, and confusion matrix for L-bend vs. non-L-bend classification.\n",
    "    - **Centroid Regression Error**: Calculates the mean, median, min, and max errors between predicted and ground-truth centroids (normalized coordinates).\n",
    "    - **Inference Speed**: Measures average inference time per image and estimates the maximum achievable FPS.\n",
    "\n",
    "- **Visualization**: Randomly selects a few validation samples and visualizes:\n",
    "    - The original image.\n",
    "    - Ground-truth centroid (green dot).\n",
    "    - Predicted centroid (red dot).\n",
    "    - A line connecting the two for error visualization.\n",
    "    - Predicted and ground-truth class labels.\n",
    "\n",
    "- **Usage**:  \n",
    "  To run the evaluation, simply execute the script with the correct model path, CSV label file, and image directory.  \n",
    "  Example:\n",
    "  ```python\n",
    "  model_path = \"best_lbend_model.pth\"  # or 'lbend_detector_scripted.pt'\n",
    "  csv_path = \"labels_my-project-name_2025-07-02-05-15-40.csv\"\n",
    "  images_dir = \"cropped_l_bends\"\n",
    "  evaluate_model(model_path, csv_path, images_dir)\n",
    "  ```\n",
    "\n",
    "This evaluation helps you understand both the classification and localization performance of your model, and provides visual feedback for qualitative assessment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34140b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "\n",
    "def evaluate_model(model_path, csv_path, images_dir):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    if model_path.endswith('.pt'):\n",
    "        # Load TorchScript model\n",
    "        model = torch.jit.load(model_path, map_location=device)\n",
    "        print(\"Loaded TorchScript model\")\n",
    "    else:\n",
    "        # Load PyTorch checkpoint\n",
    "        model = CNNDetector().to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"Loaded model checkpoint (val_loss: {checkpoint.get('val_loss', 'N/A')})\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Create validation dataset\n",
    "    val_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Split data to get validation set\n",
    "    df = pd.read_csv(csv_path, header=None)\n",
    "    train_df, val_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df[0])\n",
    "    \n",
    "    # Save val CSV temporarily\n",
    "    val_csv = 'temp_val.csv'\n",
    "    val_df.to_csv(val_csv, index=False, header=False)\n",
    "    \n",
    "    # Create dataset and loader\n",
    "    val_dataset = LBendDataset(val_csv, images_dir, transform=None)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False)  # Larger batch for faster evaluation\n",
    "    \n",
    "    # Collect predictions and ground truth\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_centroid_errors = []\n",
    "    inference_times = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for image, label, centroid in val_loader:\n",
    "            image = image.to(device)\n",
    "            label = label.to(device)\n",
    "            centroid = centroid.to(device)\n",
    "            \n",
    "            # Measure inference time\n",
    "            start_time = time.time()\n",
    "            pred_class, pred_centroid = model(image)\n",
    "            inference_times.append(time.time() - start_time)\n",
    "            \n",
    "            # Classification results\n",
    "            pred_label = (pred_class > 0.5).float()\n",
    "            all_preds.extend(pred_label.cpu().numpy())\n",
    "            all_labels.extend(label.cpu().numpy())\n",
    "            \n",
    "            # Centroid error\n",
    "            centroid_error = torch.sqrt(((pred_centroid - centroid) ** 2).sum(dim=1)).cpu().numpy()\n",
    "            all_centroid_errors.extend(centroid_error)\n",
    "    \n",
    "    # 1. Classification Report\n",
    "    all_preds = np.array(all_preds).flatten()\n",
    "    all_labels = np.array(all_labels).flatten()\n",
    "    \n",
    "    print(\"\\n--- CLASSIFICATION METRICS ---\")\n",
    "    print(classification_report(all_labels, all_preds, target_names=['Not L-bend', 'L-bend']))\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n",
    "    \n",
    "    # 2. Centroid Error Analysis\n",
    "    all_centroid_errors = np.array(all_centroid_errors)\n",
    "    \n",
    "    print(\"\\n--- CENTROID PREDICTION ERROR ---\")\n",
    "    print(f\"Mean Error (normalized): {np.mean(all_centroid_errors):.4f}\")\n",
    "    print(f\"Median Error (normalized): {np.median(all_centroid_errors):.4f}\")\n",
    "    print(f\"Min Error: {np.min(all_centroid_errors):.4f}\")\n",
    "    print(f\"Max Error: {np.max(all_centroid_errors):.4f}\")\n",
    "    \n",
    "    # 3. Inference Time Analysis\n",
    "    avg_inference_time = np.mean(inference_times) / val_loader.batch_size  # Per image\n",
    "    \n",
    "    print(\"\\n--- INFERENCE TIME ---\")\n",
    "    print(f\"Average inference time per image: {avg_inference_time*1000:.2f} ms\")\n",
    "    print(f\"Theoretical max FPS: {1/avg_inference_time:.1f}\")\n",
    "    \n",
    "    # 4. Visualization of sample predictions\n",
    "    visualize_samples(model, val_dataset, device, num_samples=9)\n",
    "    \n",
    "    # Clean up\n",
    "    if os.path.exists(val_csv):\n",
    "        os.remove(val_csv)\n",
    "\n",
    "def visualize_samples(model, dataset, device, num_samples=9):\n",
    "    \"\"\"\n",
    "    Visualize sample predictions with ground truth\n",
    "    \"\"\"\n",
    "    indices = np.random.choice(len(dataset), min(num_samples, len(dataset)), replace=False)\n",
    "    \n",
    "    # Create figure\n",
    "    num_cols = min(3, num_samples)\n",
    "    num_rows = (num_samples + num_cols - 1) // num_cols\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 4*num_rows))\n",
    "    if num_rows == 1 and num_cols == 1:\n",
    "        axes = np.array([axes])\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, idx in enumerate(indices):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "        \n",
    "        # Get data\n",
    "        image, label, centroid = dataset[idx]\n",
    "        img_name = dataset.annotations.iloc[idx]['image_name']\n",
    "        \n",
    "        # Get original image (without normalization)\n",
    "        img_path = os.path.join(dataset.images_dir, img_name)\n",
    "        orig_img = cv2.imread(img_path)\n",
    "        orig_img = cv2.cvtColor(orig_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = orig_img.shape[:2]\n",
    "        \n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            image_tensor = image.unsqueeze(0).to(device)\n",
    "            pred_class, pred_centroid = model(image_tensor)\n",
    "        \n",
    "        # Convert to values\n",
    "        gt_label = label.item()\n",
    "        gt_centroid_x, gt_centroid_y = centroid[0].item(), centroid[1].item()\n",
    "        pred_label = (pred_class > 0.5).float().item()\n",
    "        pred_prob = pred_class.item()\n",
    "        pred_centroid_x, pred_centroid_y = pred_centroid[0][0].item(), pred_centroid[0][1].item()\n",
    "        \n",
    "        # Convert normalized coordinates to pixel coordinates\n",
    "        gt_x, gt_y = int(gt_centroid_x * w), int(gt_centroid_y * h)\n",
    "        pred_x, pred_y = int(pred_centroid_x * w), int(pred_centroid_y * h)\n",
    "        \n",
    "        # Draw on image\n",
    "        vis_img = orig_img.copy()\n",
    "        \n",
    "        # Draw centroids\n",
    "        cv2.circle(vis_img, (gt_x, gt_y), 5, (0, 255, 0), -1)  # Ground truth (green)\n",
    "        cv2.circle(vis_img, (pred_x, pred_y), 5, (255, 0, 0), -1)  # Prediction (red)\n",
    "        \n",
    "        # Draw connecting line\n",
    "        cv2.line(vis_img, (gt_x, gt_y), (pred_x, pred_y), (255, 255, 0), 2)\n",
    "        \n",
    "        # Title\n",
    "        title = f\"GT: {'L-bend' if gt_label > 0.5 else 'Not L-bend'}\\n\"\n",
    "        title += f\"Pred: {'L-bend' if pred_label > 0.5 else 'Not L-bend'} ({pred_prob:.2f})\"\n",
    "        \n",
    "        # Display\n",
    "        axes[i].imshow(vis_img)\n",
    "        axes[i].set_title(title)\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    # Hide unused axes\n",
    "    for j in range(i+1, len(axes)):\n",
    "        axes[j].axis('off')\n",
    "        \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('model_evaluation.png', dpi=150)\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"best_lbend_model.pth\"  # or 'lbend_detector_scripted.pt'\n",
    "    csv_path = \"labels_my-project-name_2025-07-02-05-15-40.csv\"\n",
    "    images_dir = \"cropped_l_bends\"\n",
    "    \n",
    "    evaluate_model(model_path, csv_path, images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12decb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loaded TorchScript model\n",
      "Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "from torchvision import transforms\n",
    "\n",
    "def real_time_inference(model_path, camera_index=0, confidence_threshold=0.5):\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load model\n",
    "    try:\n",
    "        # Try loading as TorchScript model first (faster)\n",
    "        model = torch.jit.load(model_path, map_location=device)\n",
    "        print(\"Loaded TorchScript model\")\n",
    "    except:\n",
    "        # Fall back to loading PyTorch checkpoint\n",
    "        #from your_model_file import CNNDetector\n",
    "        model = CNNDetector().to(device)\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"Loaded PyTorch checkpoint\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Define transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((64, 64)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    \n",
    "    # Open camera\n",
    "    cap = cv2.VideoCapture(camera_index)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open camera {camera_index}\")\n",
    "        return\n",
    "    \n",
    "    # Get camera properties\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    \n",
    "    # For FPS calculation\n",
    "    fps = 0\n",
    "    frame_times = []\n",
    "    \n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        # Start timing\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Read frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            break\n",
    "        \n",
    "        # Convert to RGB (for preprocessing)\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Preprocess\n",
    "        input_tensor = transform(rgb_frame).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Run inference\n",
    "        with torch.no_grad():\n",
    "            class_output, centroid_output = model(input_tensor)\n",
    "        \n",
    "        # Get results\n",
    "        probability = class_output.item()\n",
    "        is_lbend = probability > confidence_threshold\n",
    "        \n",
    "        # Convert normalized centroid coordinates back to original image\n",
    "        centroid_x = int(centroid_output[0, 0].item() * frame_width)\n",
    "        centroid_y = int(centroid_output[0, 1].item() * frame_height)\n",
    "        \n",
    "        # Draw results on frame\n",
    "        if is_lbend:\n",
    "            # Draw centroid\n",
    "            cv2.circle(frame, (centroid_x, centroid_y), 5, (0, 255, 0), -1)\n",
    "            \n",
    "            # Draw box around detection\n",
    "            box_size = min(frame_width, frame_height) // 8\n",
    "            x1 = max(0, centroid_x - box_size)\n",
    "            y1 = max(0, centroid_y - box_size)\n",
    "            x2 = min(frame_width, centroid_x + box_size)\n",
    "            y2 = min(frame_height, centroid_y + box_size)\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            \n",
    "            # Show confidence\n",
    "            text = f\"L-bend: {probability:.2f}\"\n",
    "            cv2.putText(frame, text, (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        process_time = time.time() - start_time\n",
    "        frame_times.append(process_time)\n",
    "        # Keep only the last 30 frames for averaging\n",
    "        if len(frame_times) > 30:\n",
    "            frame_times.pop(0)\n",
    "        \n",
    "        fps = 1.0 / (sum(frame_times) / len(frame_times))\n",
    "        cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 255), 2)\n",
    "        \n",
    "        # Display frame\n",
    "        cv2.imshow('L-Bend Detector', frame)\n",
    "        \n",
    "        # Exit on 'q' key\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    # Release resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_path = \"lbend_detector_scripted.pt\"  # Prefer TorchScript for faster inference\n",
    "    real_time_inference(model_path, camera_index=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba2517e5",
   "metadata": {},
   "source": [
    "# Final L- Bent detection Model **Template matching RPN + centroid Prediction Model**\n",
    "\n",
    "\n",
    "This code implements a **hybrid L-bend detection pipeline** that combines a classical computer vision RPN (Region Proposal Network) using OpenCV template matching with a deep learning-based CNN for validation and centroid prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## **Pipeline Overview**\n",
    "\n",
    "1. **Template Matching RPN (Region Proposal Network)**\n",
    "    - Uses OpenCV's `cv2.matchTemplate` to scan the input image for L-bend patterns.\n",
    "    - Eight rotated versions of the template (0°, 45°, ..., 315°) are used for rotation-invariant detection.\n",
    "    - For each rotation, the template is matched to the image, and locations with high similarity scores are collected as candidate bounding boxes.\n",
    "\n",
    "2. **Non-Maximum Suppression (NMS)**\n",
    "    - Overlapping detections are filtered using NMS to keep only the best candidates.\n",
    "\n",
    "3. **CNN Validation and Centroid Prediction**\n",
    "    - Each candidate region (cropped from the image) is passed to a trained CNN.\n",
    "    - The CNN outputs:\n",
    "        - A confidence score (probability of being an L-bend).\n",
    "        - The predicted centroid coordinates (relative to the crop).\n",
    "    - Only detections with a confidence above a threshold are kept.\n",
    "\n",
    "4. **Post-processing**\n",
    "    - The predicted centroids are mapped back to the original image coordinates.\n",
    "    - The detection closest to the image center is selected as the final L-bend location (useful for robotic picking or tracking tasks).\n",
    "    - Annotated results (bounding boxes, centroids, scores, FPS) are drawn on the output frame.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why This Approach?**\n",
    "\n",
    "- **Template Matching RPN**:  \n",
    "  After evaluating several RPN methods (FFT-based, PyTorch batched convolution, OpenCV template matching), OpenCV's `matchTemplate` was chosen for its speed and reliability, especially on resource-constrained devices like Jetson Nano. The FFT and convolution approaches were either too slow or less robust in practice.\n",
    "\n",
    "- **Rotation Invariance**:  \n",
    "  By using 8 rotated templates, the system can detect L-bends at any orientation.\n",
    "\n",
    "- **CNN Validation**:  \n",
    "  The CNN filters out false positives and provides precise centroid localization, improving accuracy over classical methods alone.\n",
    "\n",
    "---\n",
    "\n",
    "## **How It Works (Step-by-Step)**\n",
    "\n",
    "1. **Initialization**:\n",
    "    - Load the template image and generate 8 rotated versions.\n",
    "    - Load the trained CNN model (TorchScript for fast inference).\n",
    "\n",
    "2. **Frame Processing**:\n",
    "    - Resize the input frame for consistent processing.\n",
    "    - Run template matching for each rotated template to get candidate boxes.\n",
    "    - Apply NMS to remove redundant detections.\n",
    "    - Extract and preprocess crops from the candidate boxes.\n",
    "\n",
    "3. **CNN Inference**:\n",
    "    - Batch all crops and run them through the CNN.\n",
    "    - For each crop, get the confidence score and centroid prediction.\n",
    "    - Keep only detections above the confidence threshold.\n",
    "\n",
    "4. **Result Aggregation**:\n",
    "    - Map predicted centroids back to the original image.\n",
    "    - Find the detection closest to the image center (if needed).\n",
    "    - Draw bounding boxes, centroids, and scores on the frame.\n",
    "    - Display FPS and total detections.\n",
    "\n",
    "---\n",
    "\n",
    "## **Advantages**\n",
    "\n",
    "- **Fast and Lightweight**: Runs in real time on Jetson Nano and similar devices.\n",
    "- **Accurate**: Combines the strengths of classical and deep learning methods.\n",
    "- **Rotation-Invariant**: Robust to object orientation.\n",
    "- **Easy to Extend**: Can be adapted for other shapes or templates.\n",
    "\n",
    "---\n",
    "\n",
    "## **Usage**\n",
    "\n",
    "- The `CombinedDetector` class encapsulates the full pipeline.\n",
    "- Use `run_real_time_detection(template_path, model_path, frame)` to process a frame and get the centroid and annotated output.\n",
    "- The main loop demonstrates real-time webcam inference.\n",
    "\n",
    "---\n",
    "\n",
    "**In summary:**  \n",
    "This final model leverages the best of both worlds: OpenCV template matching for fast, rotation-invariant region proposals, and a CNN for robust validation and precise localization. This hybrid approach was chosen after extensive experimentation and is well-suited for real-time L-bend detection in practical applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f4322",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "\n",
    "\n",
    "class CombinedDetector:\n",
    "    def __init__(self, template_path, model_path, threshold=0.52, iou_threshold=0.3, confidence_threshold=0.5):\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(f\"Using device: {self.device}\")\n",
    "\n",
    "        template_bgr = cv2.imread(template_path)\n",
    "        template_bgr = cv2.resize(template_bgr, (64, 64))\n",
    "        self.template_tensor = self.to_tensor_from_red_channel(template_bgr).to(self.device)\n",
    "\n",
    "        angles = [0, 45, 90, 135, 180, 225, 270, 315]\n",
    "        self.rotated_templates = self.get_rotated_templates(self.template_tensor, angles)\n",
    "\n",
    "        try:\n",
    "            self.model = torch.jit.load(model_path, map_location=self.device)\n",
    "            print(\"Model loaded successfully.\")\n",
    "        except:\n",
    "            print(\"Failed to load model.\")\n",
    "            raise\n",
    "\n",
    "        self.model.eval()\n",
    "        self.threshold = threshold\n",
    "        self.iou_threshold = iou_threshold\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "        self.frame_times = []\n",
    "        self.detection_count = 0\n",
    "\n",
    "    def preprocess(self,image_rgb):\n",
    "        #image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "        resized = cv2.resize(image_rgb, (64, 64)).astype(np.float32) / 255.0\n",
    "        tensor = torch.from_numpy(resized).permute(2, 0, 1)\n",
    "        return tensor\n",
    "    \n",
    "    def to_tensor_from_red_channel(self, image_bgr):\n",
    "        red_channel = image_bgr[:, :, 2].astype(np.float32) / 255\n",
    "        return torch.from_numpy(red_channel).unsqueeze(0).unsqueeze(0).contiguous()\n",
    "\n",
    "    def rotate_image_with_padding(self, image_np, angle):\n",
    "        h, w = image_np.shape\n",
    "        center = (w // 2, h // 2)\n",
    "        rot_mat = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "        cos, sin = np.abs(rot_mat[0, 0]), np.abs(rot_mat[0, 1])\n",
    "        nW, nH = int(h * sin + w * cos), int(h * cos + w * sin)\n",
    "        rot_mat[0, 2] += (nW / 2) - center[0]\n",
    "        rot_mat[1, 2] += (nH / 2) - center[1]\n",
    "        return cv2.warpAffine(image_np, rot_mat, (nW, nH), borderValue=255)\n",
    "\n",
    "    def get_rotated_templates(self, template_tensor, angles):\n",
    "        rotated_np_templates = []\n",
    "        for angle in angles:\n",
    "            template_np = template_tensor.squeeze().cpu().numpy()\n",
    "            rotated_np = self.rotate_image_with_padding(template_np, angle)\n",
    "            rotated_np_templates.append(rotated_np)\n",
    "        self.rotated_np_templates = rotated_np_templates\n",
    "        return rotated_np_templates\n",
    "\n",
    "    def match_template_opencv(self, image_np):\n",
    "        all_boxes = []\n",
    "        h, w = 64, 64\n",
    "        image_gray = (image_np[:, :, 2] / 255.0).astype(np.float32)\n",
    "        for template_np in self.rotated_np_templates:\n",
    "            result = cv2.matchTemplate(image_gray, template_np, cv2.TM_CCOEFF_NORMED)\n",
    "            yx = np.where(result >= self.threshold)\n",
    "            for y, x in zip(*yx):\n",
    "                score = result[y, x]\n",
    "                all_boxes.append((x, y, x + w, y + h, score))\n",
    "        return all_boxes\n",
    "\n",
    "    def nms_numpy(self, boxes):\n",
    "        if not boxes:\n",
    "            return []\n",
    "        boxes = np.array(boxes)\n",
    "        x1, y1, x2, y2, scores = boxes[:, 0], boxes[:, 1], boxes[:, 2], boxes[:, 3], boxes[:, 4]\n",
    "        areas = (x2 - x1) * (y2 - y1)\n",
    "        order = scores.argsort()[::-1]\n",
    "        keep = []\n",
    "\n",
    "        while order.size:\n",
    "            i = order[0]\n",
    "            keep.append(tuple(boxes[i]))\n",
    "            xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "            yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "            xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "            yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "            w = np.maximum(0.0, xx2 - xx1)\n",
    "            h = np.maximum(0.0, yy2 - yy1)\n",
    "            inter = w * h\n",
    "            iou = inter / (areas[i] + areas[order[1:]] - inter + 1e-6)\n",
    "            order = order[1:][iou <= self.iou_threshold]\n",
    "        return keep\n",
    "\n",
    "    def extract_crops(self, frame, boxes, padding=5):\n",
    "        crops = []\n",
    "        scaled_boxes = []\n",
    "        frame_h, frame_w = frame.shape[:2]\n",
    "        for x1, y1, x2, y2, _ in boxes:\n",
    "            x1_pad = max(0, int(x1 - padding))\n",
    "            y1_pad = max(0, int(y1 - padding))\n",
    "            x2_pad = min(frame_w, int(x2 + padding))\n",
    "            y2_pad = min(frame_h, int(y2 + padding))\n",
    "            crop = frame[y1_pad:y2_pad, x1_pad:x2_pad]\n",
    "            if crop.size > 0:\n",
    "                crops.append(crop)\n",
    "                scaled_boxes.append((x1_pad, y1_pad, x2_pad, y2_pad))\n",
    "        return crops, scaled_boxes\n",
    "\n",
    "    def process_with_cnn(self, crops, scaled_boxes):\n",
    "        if not crops:\n",
    "            return []\n",
    "        batch_tensors = []\n",
    "        for crop in crops:\n",
    "            rgb_crop = cv2.cvtColor(crop, cv2.COLOR_BGR2RGB)\n",
    "            tensor = self.preprocess(rgb_crop)\n",
    "            batch_tensors.append(tensor)\n",
    "\n",
    "        batch = torch.stack(batch_tensors).to(self.device)\n",
    "        with torch.no_grad():\n",
    "            class_outputs, centroid_outputs = self.model(batch)\n",
    "        detections = []\n",
    "        for i, ((x1, y1, x2, y2), class_output, centroid_output) in enumerate(zip(scaled_boxes, class_outputs, centroid_outputs)):\n",
    "            probability = class_output.item()\n",
    "            if probability > self.confidence_threshold:\n",
    "                crop_w, crop_h = x2 - x1, y2 - y1\n",
    "                crop_cx = int(centroid_output[0].item() * crop_w)\n",
    "                crop_cy = int(centroid_output[1].item() * crop_h)\n",
    "                frame_cx = x1 + crop_cx\n",
    "                frame_cy = y1 + crop_cy\n",
    "                detections.append({\n",
    "                    'bbox': (x1, y1, x2, y2),\n",
    "                    'centroid': (frame_cx, frame_cy),\n",
    "                    'probability': probability\n",
    "                })\n",
    "        return detections\n",
    "\n",
    "    def process_frame(self, frame):\n",
    "        original_h, original_w = frame.shape[:2]\n",
    "        process_size = (150, 150)\n",
    "        resized_frame = cv2.resize(frame, process_size)\n",
    "        start_time = time.time()\n",
    "        all_boxes = self.match_template_opencv(resized_frame)\n",
    "        filtered_boxes = self.nms_numpy(all_boxes)\n",
    "        crops, scaled_boxes = self.extract_crops(resized_frame, filtered_boxes)\n",
    "        detections = self.process_with_cnn(crops, scaled_boxes)\n",
    "        scale_x = original_w / process_size[0]\n",
    "        scale_y = original_h / process_size[1]\n",
    "        for detection in detections:\n",
    "            x1, y1, x2, y2 = detection['bbox']\n",
    "            cx, cy = detection['centroid']\n",
    "            detection['bbox'] = (int(x1 * scale_x), int(y1 * scale_y), int(x2 * scale_x), int(y2 * scale_y))\n",
    "            detection['centroid'] = (int(cx * scale_x), int(cy * scale_y))\n",
    "        process_time = time.time() - start_time\n",
    "        self.frame_times.append(process_time)\n",
    "        if len(self.frame_times) > 30:\n",
    "            self.frame_times.pop(0)\n",
    "        fps = 1.0 / (sum(self.frame_times) / len(self.frame_times))\n",
    "        self.detection_count += len(detections)\n",
    "        return {\n",
    "            'detections': detections,\n",
    "            'process_time': process_time,\n",
    "            'fps': fps\n",
    "        }\n",
    "    \n",
    "    def calculate_pixel_distance(self, point1, point2):\n",
    "        coord_distance = np.array(point1) - np.array(point2)\n",
    "        pixel_distance = np.linalg.norm(coord_distance)\n",
    "        return pixel_distance\n",
    "\n",
    "    def draw_results(self, frame, results,mid_point):\n",
    "        output = frame.copy()\n",
    "        min_distance_centroid = np.array([None, None])\n",
    "        for det in results['detections']:\n",
    "            x1, y1, x2, y2 = det['bbox']\n",
    "            cx, cy = det['centroid']\n",
    "            prob = det['probability']\n",
    "            cv2.rectangle(output, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            cv2.circle(output, (cx, cy), 5, (0, 0, 255), -1)\n",
    "            cv2.putText(output, f\"{prob:.2f}\", (x1, y1-5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            pixel_distance = self.calculate_pixel_distance((cx, cy), mid_point)\n",
    "            if min_distance_centroid[0] is None or pixel_distance < self.calculate_pixel_distance(min_distance_centroid, mid_point):\n",
    "                min_distance_centroid = np.array([cx, cy])\n",
    "\n",
    "                \n",
    "        cv2.putText(output, f\"FPS: {results['fps']:.1f}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 25), 2)\n",
    "        cv2.putText(output, f\"Time: {results['process_time']*1000:.1f}ms\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 25, 255), 2)\n",
    "        cv2.putText(output, f\"Total Detections: {self.detection_count}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 2, 255), 2)\n",
    "        return output,min_distance_centroid\n",
    "\n",
    "def run_real_time_detection(template_path, model_path,frame):\n",
    "\n",
    "    detector = CombinedDetector(template_path, model_path)\n",
    "    \n",
    "    frame = cv2.resize(frame, (540, 540))\n",
    "    mid_point = (frame.shape[1] // 2, frame.shape[0] // 2)\n",
    "    results = detector.process_frame(frame)\n",
    "    output,centroid = detector.draw_results(frame, results, mid_point)\n",
    "    detector.detection_count = 0\n",
    "\n",
    "    return centroid, output\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    template_path = \"image_20250321_201450_lbent_1.png\"\n",
    "    model_path = \"lbend_detector_scripted.pt\"\n",
    "    \n",
    "    # Example frame, replace with actual frame capture\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    ret, frame = cap.read()\n",
    "  \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Error: Failed to capture image\")\n",
    "            exit()\n",
    "        \n",
    "    \n",
    "        centroid, annotated_frame = run_real_time_detection(template_path, model_path, frame)\n",
    "        print(f\"Centroid Coordinates: {centroid}\")\n",
    "        cv2.imshow(\"Annotated Frame\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d04fa0",
   "metadata": {},
   "source": [
    "# Methodology and Development Process for L-Bend Detection in Bin Picking Robot\n",
    "\n",
    "## Initial Hypothesis and Frequency Domain Analysis\n",
    "\n",
    "The development of this L-bend detection system began with a fundamental hypothesis based on signal processing theory. The initial assumption was that L-shaped objects would exhibit distinct frequency characteristics that could differentiate them from background noise and other objects in a controlled environment.\n",
    "\n",
    "### Frequency Domain Investigation\n",
    "\n",
    "The first approach leveraged the principle that geometric shapes possess unique frequency signatures in the Fourier domain. The methodology involved:\n",
    "\n",
    "1. **FFT Analysis of Object Boundaries**\n",
    "   - Applied Fast Fourier Transform to extract frequency components of L-bend objects\n",
    "   - Implemented high-pass filtering to enhance edge features and suppress low-frequency background variations\n",
    "   - Generated frequency domain heat maps to visualize object boundaries\n",
    "\n",
    "2. **Contour-Based Detection Pipeline**\n",
    "   - Converted filtered frequency data to spatial heat maps\n",
    "   - Applied contour detection algorithms to extract bounding box candidates\n",
    "   - Attempted to localize objects based on boundary intensity patterns\n",
    "\n",
    "### Initial Results and Limitations\n",
    "\n",
    "<img src=\"Results/fft analysis.png\" alt=\"FFT Analysis Results\" width=\"400\"/>\n",
    "<img src=\"Results/fft_filtering_rpn.png\" alt=\"FFT Filtering RPN Results\" width=\"400\"/>\n",
    "\n",
    "The FFT-based approach yielded mixed results:\n",
    "\n",
    "**Advantages:**\n",
    "- Successfully enhanced object boundaries in clear, unoccluded scenarios\n",
    "- Provided theoretical foundation for frequency-based object discrimination\n",
    "- Demonstrated robustness to illumination variations\n",
    "\n",
    "**Critical Limitations:**\n",
    "- **Partial Occlusion Sensitivity**: The method failed to detect objects when partially occluded by other items in the bin\n",
    "- **Computational Complexity**: Real-time performance was suboptimal for robotic applications\n",
    "- **False Positive Rate**: Background textures and noise generated numerous false detections\n",
    "- **Localization Accuracy**: Contour-based bounding box extraction lacked precision required for robotic manipulation\n",
    "\n",
    "## Correlation-Based Template Matching Evolution\n",
    "\n",
    "Following the limitations of pure frequency domain analysis, the methodology evolved to incorporate correlation-based template matching while retaining FFT preprocessing benefits.\n",
    "\n",
    "### Hybrid FFT-Correlation Approach\n",
    "\n",
    "This intermediate approach combined:\n",
    "- FFT high-pass filtering for edge enhancement\n",
    "- Cross-correlation between reference template FFT and target image FFT\n",
    "- Normalized correlation coefficients for similarity scoring\n",
    "\n",
    "### Performance Assessment\n",
    "\n",
    "While this hybrid method showed marginal improvements over pure FFT analysis, it continued to exhibit:\n",
    "- Inconsistent detection rates under varying lighting conditions\n",
    "- Poor performance with cluttered backgrounds\n",
    "- Computational overhead unsuitable for real-time robotic applications\n",
    "\n",
    "## Transition to Classical Computer Vision Methods\n",
    "\n",
    "Based on the empirical evidence from FFT-based experiments, the development pivoted toward proven classical computer vision techniques with superior reliability and performance characteristics.\n",
    "\n",
    "### Comparative Analysis: OpenCV vs. Custom Convolution\n",
    "\n",
    "Two primary template matching approaches were implemented and evaluated:\n",
    "\n",
    "#### OpenCV Template Matching\n",
    "- Utilized `cv2.matchTemplate` with `TM_CCOEFF_NORMED` metric\n",
    "- Implemented rotation invariance through multiple template orientations\n",
    "- Leveraged optimized OpenCV implementations for computational efficiency\n",
    "\n",
    "#### Custom PyTorch Convolution\n",
    "- Developed batched convolution approach using `torch.nn.functional.conv2d`\n",
    "- Enabled GPU acceleration for parallel template matching\n",
    "- Implemented single-pass processing for all rotation angles\n",
    "\n",
    "### Performance Evaluation on Target Hardware\n",
    "\n",
    "Testing on NVIDIA Jetson Nano revealed critical performance differences:\n",
    "\n",
    "**OpenCV Method:**\n",
    "- Achieved consistent 15-20 FPS on Jetson Nano\n",
    "- Minimal memory footprint\n",
    "- Stable performance across varying image sizes\n",
    "- Reliable detection accuracy\n",
    "\n",
    "**Custom Convolution Method:**\n",
    "- Limited to 8-12 FPS on Jetson Nano due to GPU memory constraints\n",
    "- Higher computational overhead\n",
    "- Variable performance based on batch size\n",
    "\n",
    "### Final RPN Selection\n",
    "\n",
    "Based on empirical testing and deployment requirements, **OpenCV template matching** was selected as the primary Region Proposal Network due to:\n",
    "- Superior real-time performance on target hardware\n",
    "- Proven reliability in industrial applications\n",
    "- Consistent detection rates across diverse scenarios\n",
    "- Lower computational resource requirements\n",
    "\n",
    "## CNN Development for Validation and Localization\n",
    "\n",
    "Recognizing that template matching alone would generate false positives, a dedicated Convolutional Neural Network was developed to provide:\n",
    "1. **Binary Classification**: Validate whether detected regions contain actual L-bend objects\n",
    "2. **Centroid Regression**: Provide precise object localization for robotic manipulation\n",
    "\n",
    "### Dataset Preparation\n",
    "\n",
    "A specialized training dataset was created through:\n",
    "- **Automated Cropping**: Extracted regions around annotated L-bend objects from original images\n",
    "- **Augmentation**: Applied rotation, scaling, and color variations to improve generalization\n",
    "- **Centroid Annotation**: Manually annotated precise centroid coordinates for each object\n",
    "- **Negative Sampling**: Included non-L-bend regions to improve classification robustness\n",
    "\n",
    "### CNN Architecture Design\n",
    "\n",
    "The network architecture was designed with the following considerations:\n",
    "- **Lightweight Design**: Optimized for real-time inference on embedded hardware\n",
    "- **Dual-Head Structure**: Separate outputs for classification and regression tasks\n",
    "- **Batch Normalization**: Improved training stability and convergence\n",
    "- **Adaptive Pooling**: Ensured consistent feature dimensionality\n",
    "\n",
    "### Training Results and Validation\n",
    "\n",
    "<img src=\"final_training_results.png\" alt=\"Training Results\" width=\"500\"/>\n",
    "<img src=\"model_evaluation.png\" alt=\"Model Evaluation\" width=\"500\"/>\n",
    "\n",
    "The training process demonstrated:\n",
    "- **Convergence Stability**: Both classification and regression losses converged smoothly\n",
    "- **Generalization Performance**: Validation accuracy remained consistent with training performance\n",
    "- **Centroid Accuracy**: Mean localization error below 5% of object dimensions\n",
    "- **Real-time Capability**: Inference time under 10ms per crop on target hardware\n",
    "\n",
    "## Final System Integration\n",
    "\n",
    "The final detection pipeline integrates the validated components into a cohesive system:\n",
    "\n",
    "### Hybrid Architecture Benefits\n",
    "\n",
    "1. **Speed**: OpenCV template matching provides rapid region proposals\n",
    "2. **Accuracy**: CNN validation eliminates false positives and provides precise localization\n",
    "3. **Robustness**: Multi-stage approach handles various challenging scenarios\n",
    "4. **Scalability**: Modular design allows for easy adaptation to other object types\n",
    "\n",
    "### Performance Characteristics\n",
    "\n",
    "The final integrated system achieves:\n",
    "- **Real-time Operation**: 15-20 FPS on NVIDIA Jetson Nano\n",
    "- **High Precision**: <2% false positive rate with proper thresholding\n",
    "- **Rotation Invariance**: Reliable detection across 360-degree rotation range\n",
    "- **Deployment Ready**: Exported to TorchScript for optimized inference\n",
    "\n",
    "## Lessons Learned and Design Principles\n",
    "\n",
    "The development process yielded several key insights:\n",
    "\n",
    "### Technical Insights\n",
    "\n",
    "1. **Frequency Domain Limitations**: While theoretically sound, FFT-based methods proved insufficient for complex real-world scenarios with occlusions and varying backgrounds\n",
    "2. **Hardware-Software Co-optimization**: Algorithm selection must consider target hardware constraints, particularly in embedded robotics applications\n",
    "3. **Hybrid Approach Superiority**: Combining classical computer vision with modern deep learning leverages the strengths of both paradigms\n",
    "\n",
    "### Engineering Principles\n",
    "\n",
    "1. **Empirical Validation**: Theoretical advantages must be validated through comprehensive testing on target hardware and use cases\n",
    "2. **Incremental Development**: Systematic evaluation of individual components before integration reduces debugging complexity\n",
    "3. **Performance-Accuracy Trade-offs**: Real-time robotics applications require careful balance between detection accuracy and computational efficiency\n",
    "\n",
    "This methodical development approach, progressing from theoretical frequency analysis through empirical validation to final system integration, ensures both scientific rigor and practical deployment viability for industrial bin picking applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
